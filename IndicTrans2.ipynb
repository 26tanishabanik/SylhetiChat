{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/26tanishabanik/SylhetiChat/blob/main/IndicTrans2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Aa-nRCzPVdF"
      },
      "source": [
        "# IndicTrans2 HF Inference\n",
        "\n",
        "We provide an example notebook on how to use our IndicTrans2 models which were originally trained with the fairseq to HuggingFace transformers for inference purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfsv02IeP2It"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Please run the cells below to install the necessary dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKcYlUZYGLrt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/AI4Bharat/IndicTrans2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3vs7FkIGSxK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%cd /content/IndicTrans2/huggingface_interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddkRAXQ2Git0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n",
        "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
        "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
        "!python3 -m pip install sentencepiece\n",
        "\n",
        "!git clone https://github.com/VarunGumma/IndicTransToolkit.git\n",
        "%cd IndicTransToolkit\n",
        "!python3 -m pip install --editable ./\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjN7ub1tO33H"
      },
      "source": [
        "**IMPORTANT : Restart your run-time first and then run the cells below.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SLBIw6rQB-0"
      },
      "source": [
        "## Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/IndicTrans2/huggingface_interface/IndicTransToolkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22324oI2gzcY",
        "outputId": "886c577f-2f5e-45b7-9ab9-f9ea4402997d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/IndicTrans2/huggingface_interface/IndicTransToolkit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "from queue import Queue\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "from indicnlp.tokenize import indic_tokenize, indic_detokenize\n",
        "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
        "from sacremoses import MosesPunctNormalizer, MosesTokenizer, MosesDetokenizer\n",
        "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
        "\n",
        "\n",
        "class IndicProcessor:\n",
        "    def __init__(self, inference=True):\n",
        "        self.inference = inference\n",
        "\n",
        "        self._flores_codes = {\n",
        "            \"asm_Beng\": \"as\",\n",
        "            \"awa_Deva\": \"hi\",\n",
        "            \"ben_Beng\": \"bn\",\n",
        "            \"bho_Deva\": \"hi\",\n",
        "            \"brx_Deva\": \"hi\",\n",
        "            \"doi_Deva\": \"hi\",\n",
        "            \"eng_Latn\": \"en\",\n",
        "            \"gom_Deva\": \"kK\",\n",
        "            \"gon_Deva\": \"hi\",\n",
        "            \"guj_Gujr\": \"gu\",\n",
        "            \"hin_Deva\": \"hi\",\n",
        "            \"hne_Deva\": \"hi\",\n",
        "            \"kan_Knda\": \"kn\",\n",
        "            \"kas_Arab\": \"ur\",\n",
        "            \"kas_Deva\": \"hi\",\n",
        "            \"kha_Latn\": \"en\",\n",
        "            \"lus_Latn\": \"en\",\n",
        "            \"mag_Deva\": \"hi\",\n",
        "            \"mai_Deva\": \"hi\",\n",
        "            \"mal_Mlym\": \"ml\",\n",
        "            \"mar_Deva\": \"mr\",\n",
        "            \"mni_Beng\": \"bn\",\n",
        "            \"mni_Mtei\": \"hi\",\n",
        "            \"npi_Deva\": \"ne\",\n",
        "            \"ory_Orya\": \"or\",\n",
        "            \"pan_Guru\": \"pa\",\n",
        "            \"san_Deva\": \"hi\",\n",
        "            \"sat_Olck\": \"or\",\n",
        "            \"snd_Arab\": \"ur\",\n",
        "            \"snd_Deva\": \"hi\",\n",
        "            \"tam_Taml\": \"ta\",\n",
        "            \"tel_Telu\": \"te\",\n",
        "            \"urd_Arab\": \"ur\",\n",
        "            \"unr_Deva\": \"hi\",\n",
        "        }\n",
        "\n",
        "        self._indic_num_map = {\n",
        "            \"\\u09e6\": \"0\",\n",
        "            \"0\": \"0\",\n",
        "            \"\\u0ae6\": \"0\",\n",
        "            \"\\u0ce6\": \"0\",\n",
        "            \"\\u0966\": \"0\",\n",
        "            \"\\u0660\": \"0\",\n",
        "            \"\\uabf0\": \"0\",\n",
        "            \"\\u0b66\": \"0\",\n",
        "            \"\\u0a66\": \"0\",\n",
        "            \"\\u1c50\": \"0\",\n",
        "            \"\\u06f0\": \"0\",\n",
        "            \"\\u09e7\": \"1\",\n",
        "            \"1\": \"1\",\n",
        "            \"\\u0ae7\": \"1\",\n",
        "            \"\\u0967\": \"1\",\n",
        "            \"\\u0ce7\": \"1\",\n",
        "            \"\\u06f1\": \"1\",\n",
        "            \"\\uabf1\": \"1\",\n",
        "            \"\\u0b67\": \"1\",\n",
        "            \"\\u0a67\": \"1\",\n",
        "            \"\\u1c51\": \"1\",\n",
        "            \"\\u0c67\": \"1\",\n",
        "            \"\\u09e8\": \"2\",\n",
        "            \"2\": \"2\",\n",
        "            \"\\u0ae8\": \"2\",\n",
        "            \"\\u0968\": \"2\",\n",
        "            \"\\u0ce8\": \"2\",\n",
        "            \"\\u06f2\": \"2\",\n",
        "            \"\\uabf2\": \"2\",\n",
        "            \"\\u0b68\": \"2\",\n",
        "            \"\\u0a68\": \"2\",\n",
        "            \"\\u1c52\": \"2\",\n",
        "            \"\\u0c68\": \"2\",\n",
        "            \"\\u09e9\": \"3\",\n",
        "            \"3\": \"3\",\n",
        "            \"\\u0ae9\": \"3\",\n",
        "            \"\\u0969\": \"3\",\n",
        "            \"\\u0ce9\": \"3\",\n",
        "            \"\\u06f3\": \"3\",\n",
        "            \"\\uabf3\": \"3\",\n",
        "            \"\\u0b69\": \"3\",\n",
        "            \"\\u0a69\": \"3\",\n",
        "            \"\\u1c53\": \"3\",\n",
        "            \"\\u0c69\": \"3\",\n",
        "            \"\\u09ea\": \"4\",\n",
        "            \"4\": \"4\",\n",
        "            \"\\u0aea\": \"4\",\n",
        "            \"\\u096a\": \"4\",\n",
        "            \"\\u0cea\": \"4\",\n",
        "            \"\\u06f4\": \"4\",\n",
        "            \"\\uabf4\": \"4\",\n",
        "            \"\\u0b6a\": \"4\",\n",
        "            \"\\u0a6a\": \"4\",\n",
        "            \"\\u1c54\": \"4\",\n",
        "            \"\\u0c6a\": \"4\",\n",
        "            \"\\u09eb\": \"5\",\n",
        "            \"5\": \"5\",\n",
        "            \"\\u0aeb\": \"5\",\n",
        "            \"\\u096b\": \"5\",\n",
        "            \"\\u0ceb\": \"5\",\n",
        "            \"\\u06f5\": \"5\",\n",
        "            \"\\uabf5\": \"5\",\n",
        "            \"\\u0b6b\": \"5\",\n",
        "            \"\\u0a6b\": \"5\",\n",
        "            \"\\u1c55\": \"5\",\n",
        "            \"\\u0c6b\": \"5\",\n",
        "            \"\\u09ec\": \"6\",\n",
        "            \"6\": \"6\",\n",
        "            \"\\u0aec\": \"6\",\n",
        "            \"\\u096c\": \"6\",\n",
        "            \"\\u0cec\": \"6\",\n",
        "            \"\\u06f6\": \"6\",\n",
        "            \"\\uabf6\": \"6\",\n",
        "            \"\\u0b6c\": \"6\",\n",
        "            \"\\u0a6c\": \"6\",\n",
        "            \"\\u1c56\": \"6\",\n",
        "            \"\\u0c6c\": \"6\",\n",
        "            \"\\u09ed\": \"7\",\n",
        "            \"7\": \"7\",\n",
        "            \"\\u0aed\": \"7\",\n",
        "            \"\\u096d\": \"7\",\n",
        "            \"\\u0ced\": \"7\",\n",
        "            \"\\u06f7\": \"7\",\n",
        "            \"\\uabf7\": \"7\",\n",
        "            \"\\u0b6d\": \"7\",\n",
        "            \"\\u0a6d\": \"7\",\n",
        "            \"\\u1c57\": \"7\",\n",
        "            \"\\u0c6d\": \"7\",\n",
        "            \"\\u09ee\": \"8\",\n",
        "            \"8\": \"8\",\n",
        "            \"\\u0aee\": \"8\",\n",
        "            \"\\u096e\": \"8\",\n",
        "            \"\\u0cee\": \"8\",\n",
        "            \"\\u06f8\": \"8\",\n",
        "            \"\\uabf8\": \"8\",\n",
        "            \"\\u0b6e\": \"8\",\n",
        "            \"\\u0a6e\": \"8\",\n",
        "            \"\\u1c58\": \"8\",\n",
        "            \"\\u0c6e\": \"8\",\n",
        "            \"\\u09ef\": \"9\",\n",
        "            \"9\": \"9\",\n",
        "            \"\\u0aef\": \"9\",\n",
        "            \"\\u096f\": \"9\",\n",
        "            \"\\u0cef\": \"9\",\n",
        "            \"\\u06f9\": \"9\",\n",
        "            \"\\uabf9\": \"9\",\n",
        "            \"\\u0b6f\": \"9\",\n",
        "            \"\\u0a6f\": \"9\",\n",
        "            \"\\u1c59\": \"9\",\n",
        "            \"\\u0c6f\": \"9\",\n",
        "        }\n",
        "\n",
        "        self._placeholder_entity_maps = Queue()\n",
        "\n",
        "        self._en_tok = MosesTokenizer(lang=\"en\")\n",
        "        self._en_normalizer = MosesPunctNormalizer()\n",
        "        self._en_detok = MosesDetokenizer(lang=\"en\")\n",
        "        self._xliterator = UnicodeIndicTransliterator()\n",
        "\n",
        "        self._multispace_regex = re.compile(\"[ ]{2,}\")\n",
        "        self._digit_space_percent = re.compile(r\"(\\d) %\")\n",
        "        self._double_quot_punc = re.compile(r\"\\\"([,\\.]+)\")\n",
        "        self._digit_nbsp_digit = re.compile(r\"(\\d) (\\d)\")\n",
        "        self._end_bracket_space_punc_regex = re.compile(r\"\\) ([\\.!:?;,])\")\n",
        "\n",
        "        self._URL_PATTERN = r\"\\b(?<![\\w/.])(?:(?:https?|ftp)://)?(?:(?:[\\w-]+\\.)+(?!\\.))(?:[\\w/\\-?#&=%.]+)+(?!\\.\\w+)\\b\"\n",
        "        self._NUMERAL_PATTERN = r\"(~?\\d+\\.?\\d*\\s?%?\\s?-?\\s?~?\\d+\\.?\\d*\\s?%|~?\\d+%|\\d+[-\\/.,:']\\d+[-\\/.,:'+]\\d+(?:\\.\\d+)?|\\d+[-\\/.:'+]\\d+(?:\\.\\d+)?)\"\n",
        "        self._EMAIL_PATTERN = r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\"\n",
        "        self._OTHER_PATTERN = r\"[A-Za-z0-9]*[#|@]\\w+\"\n",
        "\n",
        "    def get_batches(self, sentences: List[str], batch_size=8):\n",
        "        for i in tqdm(range(0, len(sentences), batch_size)):\n",
        "            yield sentences[i : i + batch_size]\n",
        "\n",
        "    def _punc_norm(self, text) -> str:\n",
        "        text = (\n",
        "            text.replace(\"\\r\", \"\")\n",
        "            .replace(\"(\", \" (\")\n",
        "            .replace(\")\", \") \")\n",
        "            .replace(\"( \", \"(\")\n",
        "            .replace(\" )\", \")\")\n",
        "            .replace(\" :\", \":\")\n",
        "            .replace(\" ;\", \";\")\n",
        "            .replace(\"`\", \"'\")\n",
        "            .replace(\"„\", '\"')\n",
        "            .replace(\"“\", '\"')\n",
        "            .replace(\"”\", '\"')\n",
        "            .replace(\"–\", \"-\")\n",
        "            .replace(\"—\", \" - \")\n",
        "            .replace(\"´\", \"'\")\n",
        "            .replace(\"‘\", \"'\")\n",
        "            .replace(\"‚\", \"'\")\n",
        "            .replace(\"’\", \"'\")\n",
        "            .replace(\"''\", '\"')\n",
        "            .replace(\"´´\", '\"')\n",
        "            .replace(\"…\", \"...\")\n",
        "            .replace(\" « \", ' \"')\n",
        "            .replace(\"« \", '\"')\n",
        "            .replace(\"«\", '\"')\n",
        "            .replace(\" » \", '\" ')\n",
        "            .replace(\" »\", '\"')\n",
        "            .replace(\"»\", '\"')\n",
        "            .replace(\" %\", \"%\")\n",
        "            .replace(\"nº \", \"nº \")\n",
        "            .replace(\" :\", \":\")\n",
        "            .replace(\" ºC\", \" ºC\")\n",
        "            .replace(\" cm\", \" cm\")\n",
        "            .replace(\" ?\", \"?\")\n",
        "            .replace(\" !\", \"!\")\n",
        "            .replace(\" ;\", \";\")\n",
        "            .replace(\", \", \", \")\n",
        "        )\n",
        "\n",
        "        text = self._multispace_regex.sub(\" \", text)\n",
        "        text = self._end_bracket_space_punc_regex.sub(r\")\\1\", text)\n",
        "        text = self._digit_space_percent.sub(r\"\\1%\", text)\n",
        "        text = self._double_quot_punc.sub(r'\\1\"', text)\n",
        "        text = self._digit_nbsp_digit.sub(r\"\\1.\\2\", text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _normalize_indic_numerals(self, line: str) -> str:\n",
        "        \"\"\"\n",
        "        Normalize the numerals in Indic languages from native script to Roman script (if present).\n",
        "\n",
        "        Args:\n",
        "            line (str): an input string with Indic numerals to be normalized.\n",
        "\n",
        "        Returns:\n",
        "            str: an input string with the all Indic numerals normalized to Roman script.\n",
        "        \"\"\"\n",
        "        return \"\".join([self._indic_num_map.get(c, c) for c in line])\n",
        "\n",
        "    def _wrap_with_placeholders(self, text: str, patterns: list) -> str:\n",
        "        \"\"\"\n",
        "        Wraps substrings with matched patterns in the given text with placeholders and returns\n",
        "        the modified text along with a mapping of the placeholders to their original value.\n",
        "\n",
        "        Args:\n",
        "            text (str): an input string which needs to be wrapped with the placeholders.\n",
        "            pattern (list): list of patterns to search for in the input string.\n",
        "\n",
        "        Returns:\n",
        "            text (str): a modified text.\n",
        "        \"\"\"\n",
        "\n",
        "        serial_no = 1\n",
        "\n",
        "        placeholder_entity_map = {}\n",
        "\n",
        "        indic_failure_cases = [\n",
        "            \"آی ڈی \",\n",
        "            \"ꯑꯥꯏꯗꯤ\",\n",
        "            \"आईडी\",\n",
        "            \"आई . डी . \",\n",
        "            \"आई . डी .\",\n",
        "            \"आई. डी. \",\n",
        "            \"आई. डी.\",\n",
        "            \"आय. डी. \",\n",
        "            \"आय. डी.\",\n",
        "            \"आय . डी . \",\n",
        "            \"आय . डी .\",\n",
        "            \"ऐटि\",\n",
        "            \"آئی ڈی \",\n",
        "            \"ᱟᱭᱰᱤ ᱾\",\n",
        "            \"आयडी\",\n",
        "            \"ऐडि\",\n",
        "            \"आइडि\",\n",
        "            \"ᱟᱭᱰᱤ\",\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            matches = set(re.findall(pattern, text))\n",
        "\n",
        "            # wrap common match with placeholder tags\n",
        "            for match in matches:\n",
        "                if pattern == self._URL_PATTERN:\n",
        "                    # Avoids false positive URL matches for names with initials.\n",
        "                    if len(match.replace(\".\", \"\")) < 4:\n",
        "                        continue\n",
        "                if pattern == self._NUMERAL_PATTERN:\n",
        "                    # Short numeral patterns do not need placeholder based handling.\n",
        "                    if (\n",
        "                        len(match.replace(\" \", \"\").replace(\".\", \"\").replace(\":\", \"\"))\n",
        "                        < 4\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                # Set of Translations of \"ID\" in all the suppported languages have been collated.\n",
        "                # This has been added to deal with edge cases where placeholders might get translated.\n",
        "                base_placeholder = f\"<ID{serial_no}>\"\n",
        "\n",
        "                placeholder_entity_map[f\"<ID{serial_no}]\"] = match\n",
        "                placeholder_entity_map[f\"< ID{serial_no} ]\"] = match\n",
        "                placeholder_entity_map[f\"<ID{serial_no}>\"] = match\n",
        "                placeholder_entity_map[f\"< ID{serial_no} >\"] = match\n",
        "                placeholder_entity_map[f\"[ID{serial_no}]\"] = match\n",
        "                placeholder_entity_map[f\"[ID {serial_no}]\"] = match\n",
        "                placeholder_entity_map[f\"[ ID{serial_no} ]\"] = match\n",
        "\n",
        "                for i in indic_failure_cases:\n",
        "                    placeholder_entity_map[f\"<{i}{serial_no}>\"] = match\n",
        "                    placeholder_entity_map[f\"< {i}{serial_no} >\"] = match\n",
        "                    placeholder_entity_map[f\"< {i} {serial_no} >\"] = match\n",
        "                    placeholder_entity_map[f\"<{i} {serial_no}]\"] = match\n",
        "                    placeholder_entity_map[f\"< {i} {serial_no} ]\"] = match\n",
        "                    placeholder_entity_map[f\"[{i}{serial_no}]\"] = match\n",
        "                    placeholder_entity_map[f\"[{i} {serial_no}]\"] = match\n",
        "                    placeholder_entity_map[f\"[ {i}{serial_no} ]\"] = match\n",
        "                    placeholder_entity_map[f\"[ {i} {serial_no} ]\"] = match\n",
        "                    placeholder_entity_map[f\"{i} {serial_no}\"] = match\n",
        "                    placeholder_entity_map[f\"{i}{serial_no}\"] = match\n",
        "\n",
        "                text = text.replace(match, base_placeholder)\n",
        "                serial_no += 1\n",
        "\n",
        "        text = re.sub(r\"\\s+\", \" \", text).replace(\">/\", \">\").replace(\"]/\", \"]\")\n",
        "        self._placeholder_entity_maps.put(placeholder_entity_map)\n",
        "        return text\n",
        "\n",
        "    def _normalize(\n",
        "        self,\n",
        "        text: str,\n",
        "    ) -> Tuple[str, dict]:\n",
        "        \"\"\"\n",
        "        Normalizes and wraps the spans of input string with placeholder tags. It first normalizes\n",
        "        the Indic numerals in the input string to Roman script. Later, it uses the input string with normalized\n",
        "        Indic numerals to wrap the spans of text matching the pattern with placeholder tags.\n",
        "\n",
        "        Args:\n",
        "            text (str): input string.\n",
        "            pattern (list): list of patterns to search for in the input string.\n",
        "\n",
        "        Returns:\n",
        "            text (str): the modified text\n",
        "        \"\"\"\n",
        "        patterns = [\n",
        "            self._EMAIL_PATTERN,\n",
        "            self._URL_PATTERN,\n",
        "            self._NUMERAL_PATTERN,\n",
        "            self._OTHER_PATTERN,\n",
        "        ]\n",
        "\n",
        "        text = self._normalize_indic_numerals(text.strip())\n",
        "\n",
        "        if self.inference:\n",
        "            text = self._wrap_with_placeholders(text, patterns)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _apply_lang_tags(\n",
        "        self, sent: str, src_lang: str, tgt_lang: str, delimiter=\" \"\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Add special tokens indicating source and target language to the start of the each input sentence.\n",
        "        Each resulting input sentence will have the format: \"`{src_lang} {tgt_lang} {input_sentence}`\".\n",
        "\n",
        "        Args:\n",
        "            sent (str): input sentence to be translated.\n",
        "            src_lang (str): flores lang code of the input sentence.\n",
        "            tgt_lang (str): flores lang code in which the input sentence will be translated.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: list of input sentences with the special tokens added to the start.\n",
        "        \"\"\"\n",
        "        return f\"{src_lang}{delimiter}{tgt_lang}{delimiter}{sent.strip()}\"\n",
        "\n",
        "    def _preprocess(\n",
        "        self,\n",
        "        sent: str,\n",
        "        lang: str,\n",
        "        normalizer: Union[MosesPunctNormalizer, IndicNormalizerFactory],\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Preprocess an input text sentence by normalizing, tokenization, and possibly transliterating it.\n",
        "\n",
        "        Args:\n",
        "            sent (str): input text sentence to preprocess.\n",
        "            normalizer (Union[MosesPunctNormalizer, IndicNormalizerFactory]): an object that performs normalization on the text.\n",
        "            lang (str): flores language code of the input text sentence.\n",
        "\n",
        "        Returns:\n",
        "            sent (str): a preprocessed input text sentence\n",
        "        \"\"\"\n",
        "        iso_lang = self._flores_codes.get(lang, \"hi\")\n",
        "        sent = self._punc_norm(sent)\n",
        "        sent = self._normalize(sent)\n",
        "\n",
        "        transliterate = True\n",
        "        if lang.split(\"_\")[1] in [\"Arab\", \"Aran\", \"Olck\", \"Mtei\", \"Latn\"]:\n",
        "            transliterate = False\n",
        "\n",
        "        if iso_lang == \"en\":\n",
        "            processed_sent = \" \".join(\n",
        "                self._en_tok.tokenize(\n",
        "                    self._en_normalizer.normalize(sent.strip()), escape=False\n",
        "                )\n",
        "            )\n",
        "        elif transliterate:\n",
        "            processed_sent = self._xliterator.transliterate(\n",
        "                \" \".join(\n",
        "                    indic_tokenize.trivial_tokenize(\n",
        "                        normalizer.normalize(sent.strip()), iso_lang\n",
        "                    )\n",
        "                ),\n",
        "                iso_lang,\n",
        "                \"hi\",\n",
        "            ).replace(\" ् \", \"्\")\n",
        "        else:\n",
        "            processed_sent = \" \".join(\n",
        "                indic_tokenize.trivial_tokenize(\n",
        "                    normalizer.normalize(sent.strip()), iso_lang\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return processed_sent\n",
        "\n",
        "    def preprocess_batch(\n",
        "        self,\n",
        "        batch: List[str],\n",
        "        src_lang: str,\n",
        "        tgt_lang: str,\n",
        "        is_target: bool = False,\n",
        "    ) -> List[str]:\n",
        "        \"\"\"\n",
        "        Preprocess an array of sentences by normalizing, tokenization, and possibly transliterating it. It also tokenizes the\n",
        "        normalized text sequences using sentence piece tokenizer and also adds language tags.\n",
        "\n",
        "        Args:\n",
        "            batch (List[str]): input list of sentences to preprocess.\n",
        "            src_lang (str): flores language code of the input text sentences.\n",
        "            tgt_lang (str): flores language code of the output text sentences.\n",
        "            is_target (bool): add language tags if false otherwise skip it.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: a list of preprocessed input text sentences.\n",
        "        \"\"\"\n",
        "        normalizer = (\n",
        "            IndicNormalizerFactory().get_normalizer(self._flores_codes.get(src_lang, \"hi\"))\n",
        "            if src_lang != \"eng_Latn\"\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        preprocessed_sents = [\n",
        "            self._preprocess(sent, src_lang, normalizer) for sent in batch\n",
        "        ]\n",
        "\n",
        "        tagged_sents = (\n",
        "            [\n",
        "                self._apply_lang_tags(sent, src_lang, tgt_lang)\n",
        "                for sent in preprocessed_sents\n",
        "            ]\n",
        "            if not is_target\n",
        "            else preprocessed_sents\n",
        "        )\n",
        "\n",
        "        return tagged_sents\n",
        "\n",
        "    def _postprocess(\n",
        "        self,\n",
        "        sent: str,\n",
        "        lang: str = \"hin_Deva\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Postprocesses a single input sentence after the translation generation.\n",
        "\n",
        "        Args:\n",
        "            sent (str): input sentence to postprocess.\n",
        "            placeholder_entity_map (dict): dictionary mapping placeholders to the original entity values.\n",
        "            lang (str): flores language code of the input sentence.\n",
        "\n",
        "        Returns:\n",
        "            text (str): postprocessed input sentence.\n",
        "        \"\"\"\n",
        "        placeholder_entity_map = self._placeholder_entity_maps.get()\n",
        "\n",
        "        if isinstance(sent, tuple) or isinstance(sent, list):\n",
        "            sent = sent[0]\n",
        "\n",
        "        lang_code, script_code = lang.split(\"_\")\n",
        "        iso_lang = self._flores_codes.get(lang, \"hi\")\n",
        "\n",
        "        # Fixes for Perso-Arabic scripts\n",
        "        if script_code in [\"Arab\", \"Aran\"]:\n",
        "            sent = (\n",
        "                sent.replace(\" ؟\", \"؟\")\n",
        "                .replace(\" ۔\", \"۔\")\n",
        "                .replace(\" ،\", \"،\")\n",
        "                .replace(\"ٮ۪\", \"ؠ\")\n",
        "            )\n",
        "\n",
        "        if lang_code == \"ory\":\n",
        "            sent = sent.replace(\"ଯ଼\", \"ୟ\")\n",
        "\n",
        "        for k, v in placeholder_entity_map.items():\n",
        "            sent = sent.replace(k, v)\n",
        "\n",
        "        return (\n",
        "            self._en_detok.detokenize(sent.split(\" \"))\n",
        "            if lang == \"eng_Latn\"\n",
        "            else indic_detokenize.trivial_detokenize(\n",
        "                self._xliterator.transliterate(sent, \"hi\", iso_lang),\n",
        "                iso_lang,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def postprocess_batch(self, sents: List[str], lang: str = \"hin_Deva\") -> List[str]:\n",
        "        \"\"\"\n",
        "        Postprocesses a batch of input sentences after the translation generations.\n",
        "\n",
        "        Args:\n",
        "            sents (List[str]): batch of translated sentences to postprocess.\n",
        "            placeholder_entity_map (List[dict]): dictionary mapping placeholders to the original entity values.\n",
        "            lang (str): flores language code of the input sentences.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: postprocessed batch of input sentences.\n",
        "        \"\"\"\n",
        "\n",
        "        postprocessed_sents = [self._postprocess(sent, lang) for sent in zip(sents)]\n",
        "\n",
        "        # for good reason, empty the placeholder entity map after each batch\n",
        "        self._placeholder_entity_maps.queue.clear()\n",
        "\n",
        "        return postprocessed_sents\n"
      ],
      "metadata": {
        "id": "B6avVzUAg_ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYczM2U6G1Zv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "quantization = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj1WCNjuHG-d"
      },
      "outputs": [],
      "source": [
        "def initialize_model_and_tokenizer(ckpt_dir, quantization):\n",
        "    if quantization == \"4-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    elif quantization == \"8-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            bnb_8bit_use_double_quant=True,\n",
        "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    else:\n",
        "        qconfig = None\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        ckpt_dir,\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "        quantization_config=qconfig,\n",
        "    )\n",
        "\n",
        "    if qconfig == None:\n",
        "        model = model.to(DEVICE)\n",
        "        if DEVICE == \"cuda\":\n",
        "            model.half()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
        "    translations = []\n",
        "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
        "        batch = input_sentences[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Preprocess the batch and extract entity mappings\n",
        "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "        # Tokenize the batch and generate input encodings\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Generate translations using the model\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                use_cache=True,\n",
        "                min_length=0,\n",
        "                max_length=256,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=1,\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens into text\n",
        "\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            generated_tokens = tokenizer.batch_decode(\n",
        "                generated_tokens.detach().cpu().tolist(),\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True,\n",
        "            )\n",
        "\n",
        "        # Postprocess the translations, including entity replacement\n",
        "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
        "\n",
        "        del inputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return translations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erNCuZTEMt49"
      },
      "source": [
        "### Bengali to English Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OG3Bw-sHnf3"
      },
      "outputs": [],
      "source": [
        "en_indic_ckpt_dir = \"ai4bharat/indictrans2-indic-en-1B\"  #\"ai4bharat/indictrans2-indic-indic-1B\"\n",
        "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, quantization)\n",
        "\n",
        "ip = IndicProcessor(inference=True)\n",
        "\n",
        "en_sents = [\n",
        "    \"আইজ দুফুর ওর খানির লাগি পানশিত ৩ জনর লগি এখটা টেবিল বুক দেও\",\n",
        "    \"জসীম ওর ইনো এখন আবহাওয়া কিলান?\"\n",
        "]\n",
        "\n",
        "src_lang, tgt_lang = \"ben_Beng\", \"eng_Latn\"\n",
        "hi_translations = batch_translate(en_sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n",
        "\n",
        "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
        "for input_sentence, translation in zip(en_sents, hi_translations):\n",
        "    print(f\"{src_lang}: {input_sentence}\")\n",
        "    print(f\"{tgt_lang}: {translation}\")\n",
        "\n",
        "# flush the models to free the GPU memory\n",
        "del en_indic_tokenizer, en_indic_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM_1pbPtMpV9"
      },
      "source": [
        "### Indic to English Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLCEWJKvGG9I",
        "outputId": "ab9d8726-67c7-490b-ecb3-208df1c0f741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "hin_Deva - eng_Latn\n",
            "hin_Deva: जब मैं छोटा था, मैं हर रोज़ पार्क जाता था।\n",
            "eng_Latn: When I was young, I used to go to the park every day.\n",
            "hin_Deva: उसके पास बहुत सारी पुरानी किताबें हैं, जिन्हें उसने अपने दादा-परदादा से विरासत में पाया।\n",
            "eng_Latn: She has a lot of old books, which she inherited from her grandparents.\n",
            "hin_Deva: मुझे समझ में नहीं आ रहा कि मैं अपनी समस्या का समाधान कैसे ढूंढूं।\n",
            "eng_Latn: I don't know how to find a solution to my problem.\n",
            "hin_Deva: वह बहुत मेहनती और समझदार है, इसलिए उसे सभी अच्छे मार्क्स मिले।\n",
            "eng_Latn: He is very hardworking and understanding, so he got all the good marks.\n",
            "hin_Deva: हमने पिछले सप्ताह एक नई फिल्म देखी जो कि बहुत प्रेरणादायक थी।\n",
            "eng_Latn: We saw a new movie last week that was very inspiring.\n",
            "hin_Deva: अगर तुम मुझे उस समय पास मिलते, तो हम बाहर खाना खाने चलते।\n",
            "eng_Latn: If you'd given me a pass at that time, we'd have gone out to eat.\n",
            "hin_Deva: वह अपनी दीदी के साथ बाजार गयी थी ताकि वह नई साड़ी खरीद सके।\n",
            "eng_Latn: She had gone to the market with her sister so that she could buy a new sari.\n",
            "hin_Deva: राज ने मुझसे कहा कि वह अगले महीने अपनी नानी के घर जा रहा है।\n",
            "eng_Latn: Raj told me that he was going to his grandmother's house next month.\n",
            "hin_Deva: सभी बच्चे पार्टी में मज़ा कर रहे थे और खूब सारी मिठाइयाँ खा रहे थे।\n",
            "eng_Latn: All the children were having fun at the party and eating a lot of sweets.\n",
            "hin_Deva: मेरे मित्र ने मुझे उसके जन्मदिन की पार्टी में बुलाया है, और मैं उसे एक तोहफा दूंगा।\n",
            "eng_Latn: My friend has invited me to her birthday party, and I'll give her a present.\n"
          ]
        }
      ],
      "source": [
        "indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-1B\"  # ai4bharat/indictrans2-indic-en-dist-200M\n",
        "indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(indic_en_ckpt_dir, quantization)\n",
        "\n",
        "ip = IndicProcessor(inference=True)\n",
        "\n",
        "hi_sents = [\n",
        "    \"जब मैं छोटा था, मैं हर रोज़ पार्क जाता था।\",\n",
        "    \"उसके पास बहुत सारी पुरानी किताबें हैं, जिन्हें उसने अपने दादा-परदादा से विरासत में पाया।\",\n",
        "    \"मुझे समझ में नहीं आ रहा कि मैं अपनी समस्या का समाधान कैसे ढूंढूं।\",\n",
        "    \"वह बहुत मेहनती और समझदार है, इसलिए उसे सभी अच्छे मार्क्स मिले।\",\n",
        "    \"हमने पिछले सप्ताह एक नई फिल्म देखी जो कि बहुत प्रेरणादायक थी।\",\n",
        "    \"अगर तुम मुझे उस समय पास मिलते, तो हम बाहर खाना खाने चलते।\",\n",
        "    \"वह अपनी दीदी के साथ बाजार गयी थी ताकि वह नई साड़ी खरीद सके।\",\n",
        "    \"राज ने मुझसे कहा कि वह अगले महीने अपनी नानी के घर जा रहा है।\",\n",
        "    \"सभी बच्चे पार्टी में मज़ा कर रहे थे और खूब सारी मिठाइयाँ खा रहे थे।\",\n",
        "    \"मेरे मित्र ने मुझे उसके जन्मदिन की पार्टी में बुलाया है, और मैं उसे एक तोहफा दूंगा।\",\n",
        "]\n",
        "src_lang, tgt_lang = \"hin_Deva\", \"eng_Latn\"\n",
        "en_translations = batch_translate(hi_sents, src_lang, tgt_lang, indic_en_model, indic_en_tokenizer, ip)\n",
        "\n",
        "\n",
        "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
        "for input_sentence, translation in zip(hi_sents, en_translations):\n",
        "    print(f\"{src_lang}: {input_sentence}\")\n",
        "    print(f\"{tgt_lang}: {translation}\")\n",
        "\n",
        "# flush the models to free the GPU memory\n",
        "del indic_en_tokenizer, indic_en_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VCAkyKBGtnV"
      },
      "source": [
        "### Indic to Indic Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7TxTTCoKjti",
        "outputId": "df1a750b-0f32-478d-cfc9-e445f669f3ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "hin_Deva - mar_Deva\n",
            "hin_Deva: जब मैं छोटा था, मैं हर रोज़ पार्क जाता था।\n",
            "mar_Deva: मी लहान होतो तेव्हा मी दररोज उद्यानाला जायचे.\n",
            "hin_Deva: उसके पास बहुत सारी पुरानी किताबें हैं, जिन्हें उसने अपने दादा-परदादा से विरासत में पाया।\n",
            "mar_Deva: तिच्याकडे बरीच जुनी पुस्तके आहेत, जी तिला तिच्या आजोबांकडून वारशाने मिळाली आहेत.\n",
            "hin_Deva: मुझे समझ में नहीं आ रहा कि मैं अपनी समस्या का समाधान कैसे ढूंढूं।\n",
            "mar_Deva: माझ्या समस्येवर तोडगा कसा काढायचा हे मला समजत नाही.\n",
            "hin_Deva: वह बहुत मेहनती और समझदार है, इसलिए उसे सभी अच्छे मार्क्स मिले।\n",
            "mar_Deva: तो खूप मेहनती आणि बुद्धिमान आहे, त्यामुळे त्याला सर्व चांगले गुण मिळाले.\n",
            "hin_Deva: हमने पिछले सप्ताह एक नई फिल्म देखी जो कि बहुत प्रेरणादायक थी।\n",
            "mar_Deva: आम्ही गेल्या आठवड्यात एक नवीन चित्रपट पाहिला जो खूप प्रेरणादायी होता.\n",
            "hin_Deva: अगर तुम मुझे उस समय पास मिलते, तो हम बाहर खाना खाने चलते।\n",
            "mar_Deva: जर तुम्हाला त्या वेळी मला पास मिळाला तर आम्ही बाहेर जेवायला जाऊ.\n",
            "hin_Deva: वह अपनी दीदी के साथ बाजार गयी थी ताकि वह नई साड़ी खरीद सके।\n",
            "mar_Deva: ती तिच्या बहिणीसोबत बाजारात गेली होती जेणेकरून ती नवीन साडी खरेदी करू शकेल.\n",
            "hin_Deva: राज ने मुझसे कहा कि वह अगले महीने अपनी नानी के घर जा रहा है।\n",
            "mar_Deva: राजने मला सांगितले की तो पुढच्या महिन्यात त्याच्या आजीच्या घरी जात आहे.\n",
            "hin_Deva: सभी बच्चे पार्टी में मज़ा कर रहे थे और खूब सारी मिठाइयाँ खा रहे थे।\n",
            "mar_Deva: सर्व मुले पार्टीचा आनंद घेत होती आणि भरपूर मिठाई खात होती.\n",
            "hin_Deva: मेरे मित्र ने मुझे उसके जन्मदिन की पार्टी में बुलाया है, और मैं उसे एक तोहफा दूंगा।\n",
            "mar_Deva: माझ्या मित्राने मला त्याच्या वाढदिवसाच्या मेजवानीसाठी आमंत्रित केले आहे आणि मी त्याला भेटवस्तू देईन.\n"
          ]
        }
      ],
      "source": [
        "indic_indic_ckpt_dir = \"ai4bharat/indictrans2-indic-indic-1B\"  # ai4bharat/indictrans2-indic-indic-dist-320M\n",
        "indic_indic_tokenizer, indic_indic_model = initialize_model_and_tokenizer(indic_indic_ckpt_dir, quantization)\n",
        "\n",
        "ip = IndicProcessor(inference=True)\n",
        "\n",
        "hi_sents = [\n",
        "    \"जब मैं छोटा था, मैं हर रोज़ पार्क जाता था।\",\n",
        "    \"उसके पास बहुत सारी पुरानी किताबें हैं, जिन्हें उसने अपने दादा-परदादा से विरासत में पाया।\",\n",
        "    \"मुझे समझ में नहीं आ रहा कि मैं अपनी समस्या का समाधान कैसे ढूंढूं।\",\n",
        "    \"वह बहुत मेहनती और समझदार है, इसलिए उसे सभी अच्छे मार्क्स मिले।\",\n",
        "    \"हमने पिछले सप्ताह एक नई फिल्म देखी जो कि बहुत प्रेरणादायक थी।\",\n",
        "    \"अगर तुम मुझे उस समय पास मिलते, तो हम बाहर खाना खाने चलते।\",\n",
        "    \"वह अपनी दीदी के साथ बाजार गयी थी ताकि वह नई साड़ी खरीद सके।\",\n",
        "    \"राज ने मुझसे कहा कि वह अगले महीने अपनी नानी के घर जा रहा है।\",\n",
        "    \"सभी बच्चे पार्टी में मज़ा कर रहे थे और खूब सारी मिठाइयाँ खा रहे थे।\",\n",
        "    \"मेरे मित्र ने मुझे उसके जन्मदिन की पार्टी में बुलाया है, और मैं उसे एक तोहफा दूंगा।\",\n",
        "]\n",
        "src_lang, tgt_lang = \"hin_Deva\", \"mar_Deva\"\n",
        "mr_translations = batch_translate(hi_sents, src_lang, tgt_lang, indic_indic_model, indic_indic_tokenizer, ip)\n",
        "\n",
        "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
        "for input_sentence, translation in zip(hi_sents, mr_translations):\n",
        "    print(f\"{src_lang}: {input_sentence}\")\n",
        "    print(f\"{tgt_lang}: {translation}\")\n",
        "\n",
        "# flush the models to free the GPU memory\n",
        "del indic_indic_tokenizer, indic_indic_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyxXpt--Ma6n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}